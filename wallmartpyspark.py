# -*- coding: utf-8 -*-
"""wallmartPyspark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQPBKq67I0xmHt26CVb06RFAWQBkTBmv

<h1><center>Introduction to Google Colab and PySpark</center></h1>

<a id='objective'></a>
## Objective
The objective of this notebook is to:
><li>Give a proper understanding about the different PySpark functions available. </li>
><li>A short introduction to Google Colab, as that is the platform on which this notebook is written on. </li>

Once you complete this notebook, you should be able to write pyspark programs in an efficent way. The ideal way to use this is by going through the examples given and then trying them on Colab. At the end there are a few hands on questions which you can use to evaluate yourself.

<a id='prerequisite'></a>
## Prerequisite
><li>Although some theory about pyspark and big data will be given in this notebook, I recommend everyone to read more about it and have a deeper understanding on how the functions get executed and the relevance of big data in the current scenario.
><li>A good understanding on python will be an added bonus.

<a id='notes-from-the-author'></a>
## Notes from the Author

This tutorial was made using Google Colab so the code you see here is meant to run on a colab notebook. <br>
It goes through basic [PySpark Functions](https://spark.apache.org/docs/latest/api/python/index.html) and a short introduction on how to use [Colab](https://colab.research.google.com/notebooks/basic_features_overview.ipynb). <br>
If you want to view my colab notebook for this particular tutorial, you can view it [here](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc). The viewing experience and readability is much better there. <br>
If you want to try out things with this notebook as a base, feel free to download it from my repo [here](https://github.com/jacobceles/knowledge-repo/blob/master/pyspark/Colab%20and%20PySpark.ipynb) and then use it with jupyter notebook.

<a id='big-data-pyspark-and-colaboratory'></a>
## Big data, PySpark and Colaboratory

<a id='big-data'></a>
### Big data

Big data usually means data of such huge volume that normal data storage solutions cannot efficently store and process it. In this era, data is being generated at an absurd rate. Data is collected for each movement a person makes. The bulk of big data comes from three primary sources:
<ol>
   <li>Social data</li>
   <li>Machine data</li>
   <li>Transactional data</li>
</ol>

Some common examples for the sources of such data include internet searches, facebook posts, doorbell cams, smartwatches, online shopping history etc. Every action creates data, it is just a matter of of there is a way to collect them or not.  But what's interesting is that out of all this data collected, not even 5% of it is being used fully. There is a huge demand for big data professionals in the industry. Even though the number of graduates with a specialization in big data are rising, the problem is that they don't have the practical knowledge about big data scenarios, which leads to bad architecutres and inefficent methods of processing data.

>If you are interested to know more about the landscape and technologies involved, here is [an article](https://hostingtribunal.com/blog/big-data-stats/) which I found really interesting!

<a id='pyspark'></a>
### PySpark

If you are working in the field of big data, you must have definelty heard of spark. If you look at the [Apache Spark](https://spark.apache.org/) website, you will see that it is said to be a `Lightning-fast unified analytics engine`. PySpark is a flavour of Spark used for processing and analysing massive volumes of data. If you are familiar with python and have tried it for huge datasets, you should know that the execution time can get ridiculous. Enter PySpark!

Imagine your data resides in a distributed manner at different places. If you try brining your data to one point and executing your code there, not only would that be inefficent, but also cause memory issues. Now let's say your code goes to the data rather than the data coming to where your code. This will help avoid unneccesary data movement which will thereby decrease the running time.

PySpark is the Python API of Spark; which means it can do almost all the things python can. Machine learning(ML) pipelines, exploratory data analysis (at scale), ETLs for data platform, and much more! And all of them in a distributed manner. One of the best parts of pyspark is that if you are already familiar with python, it's really easy to learn.

Apart from PySpark, there is another language called Scala used for big data processing. Scala is frequently over 10 times faster than *Python*, as it is native for Hadoop as its based on JVM. But PySpark is getting adopted at a fast rate because of the ease of use, easier learning curve and ML capabilities.

I will briefly explain how a PySpark job works, but I strongly recommend you read more about the [architecture](https://data-flair.training/blogs/how-apache-spark-works/) and how everything works. Now, before I get into it, let me talk about some <u>basic jargons</u> first:

<b>Cluster</b> is a set of loosely or tightly connected computers that work together so that they can be viewed as a single system.

<b>Hadoop</b> is an open source, scalable, and fault tolerant framework written in Java. It efficiently processes large volumes of data on a cluster of commodity hardware. Hadoop is not only a storage system but is a platform for large data storage as well as processing.

<b>HDFS</b> (Hadoop distributed file system). It is one of the world's most reliable storage system. HDFS is a Filesystem of Hadoop designed for storing very large files running on a cluster of commodity hardware.

<b>MapReduce</b> is a data Processing framework, which has 2 phases - Mapper and Reducer. The map procedure performs filtering and sorting, and the reduce method performs a summary operation. It usually runs on a hadoop cluster.

<b>Transformation</b> refers to the operations applied on a dataset to create a new dataset. Filter, groupBy and map are the examples of transformations.

<b>Actions</b> Actions refer to an operation which instructs Spark to perform computation and send the result back to driver. This is an example of action.

Alright! Now that that's out of the way, let me explain how a spark job runs. In simple terma, each time you submit a pyspark job, the code gets internally converted into a MapReduce program and gets executed in the Java virtual machine. Now one of the thoughts that might be popping in your mind will probably be: <br>`So the code gets converted into a MapReduce program. Wouldn't that mean MapReduce is faster than pySpark?`<br> Well, the answer is a big NO. This is what makes spark jobs special. Spark is capable of handling a massive amount of data at a time, in it's distributed environment. It does this through <u>in-memory processing</u>, which is what makes it almost 100 times faster than Hadoop. Another factor which amkes it fast is <u>Lazy Evaluation</u>. Spark delays its evaluation as much as it can. Each time you  submit a job, spark creates an action plan for how to execute the code, and then does nothing. Finally, when you ask for the result(i.e, calls an action), it executes the plan, which is basically all the transofrmations you have mentioned in your code. That's basically the gist of it.

Now lastly, I want to talk about on more thing. Spark mainly consists of 4 modules:

<ol>
    <li>Spark SQL - helps to write  spark programs using SQL like queries.</li>
    <li>Spark Streaming - is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. used heavily in processing of social media data.</li>
    <li>Spark MLLib - is the machine learning component of SPark. It helps train ML models on massive datasets with very high efficeny. </li>
    <li>Spark GraphX - is the visualization component of Spark. It enables users to view data both as graphs and as collections without data movement or duplication.</li>
</ol>

Hopefully this image gives a better idea of what I am talking about:
<img alt="Spark Modules" src="https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2015/11/spark-streaming-datanami.png" />
<center><font color='#666956'>Source: Datanami</font><center>

<a id='colaboratory'></a>
### Colaboratory

In the words of Google: <br>
`Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.`

The reason why I used colab is because of its shareability and free GPU and TPU. Yeah you read that right, FREE GPU AND TPU! For using TPU, your program needs to be optimized for the same. Additionally, it helps use different Google services conveniently. It saves to Google Drive and all the services are very closely related. I recommend you go through the offical [overview documentation](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) if you want to know more about it.
If you have more questions about colab, please [refer this link](https://research.google.com/colaboratory/faq.html).

>While using a colab notebook, you will need an active internet connection to keep a session alive. If you lose the connection you will have to download the datasets again.

<a id='jupyter-notebook-basics'></a>
## Jupyter notebook basics

<a id='access-to-the-shell'></a>
### Access to the shell
"""

ls

pwd

"""<a id='installing-spark'></a>
### Installing Spark

Install Dependencies:


1.   Java 8
2.   Apache Spark with hadoop and
3.   Findspark (used to locate the spark in the system)
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

"""Set Environment Variables:"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

!ls

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

df_features = spark.read.csv('/content/features.csv',header=True,inferSchema=True)
df_stores = spark.read.csv('/content/stores.csv',header=True,inferSchema=True)
df_test = spark.read.csv('/content/test.csv',header=True,inferSchema=True)
df_train = spark.read.csv('/content/train.csv',header=True,inferSchema=True)

df_features.count(), df_features.columns

df_stores.count(), df_stores.columns

df_test.count(), df_test.columns

df_train.count(), df_train.columns

df_features.printSchema()
df_stores.printSchema()
df_test.printSchema()
df_train.printSchema()

df_features.describe().show()
df_stores.describe().show()
df_test.describe().show()
df_train.describe().show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pyspark.sql.functions import count, when, isnull

df_combined1 = df_train.join(df_stores, on='Store', how='inner').join(df_features.drop('IsHoliday'), on=['Store', 'Date'], how='inner')
df_combined1

from pyspark.sql.functions import col, count, when

# Contar los valores nulos en cada columna
df_combined1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_combined1.columns]).show()

import numpy as np
np.bool = np.bool_

full_df = df_combined1.toPandas()

full_df['Date'] = pd.to_datetime(full_df['Date'])
full_df.set_index('Date', inplace=True)
full_df['CPI'] = pd.to_numeric(full_df['CPI'], downcast='float')
full_df['Unemployment'] = pd.to_numeric(full_df['Unemployment'], downcast='float')
full_df.head()

full_dff = full_df.fillna(0)
full_dff.head()

# Convertir las columnas de MarkDown a numéricas, reemplazando errores con NaN
for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:
    full_dff[col] = pd.to_numeric(full_dff[col], errors='coerce')

# Rellenar valores nulos con 0
full_dff.fillna(0, inplace=True)

full_dff.replace('NA', np.nan, inplace=True)
full_dff.head()

full_dff = full_dff.fillna(0)
full_dff.head()

full_dff.info()

#1
plt.figure(figsize=(10, 6))
sns.histplot(full_dff['Weekly_Sales'], bins=30, kde=True)
plt.title('Distribución de Ventas Semanales')
plt.xlabel('Ventas Semanales')
plt.ylabel('Frecuencia')
plt.show()

#2
plt.figure(figsize=(10, 6))
sns.boxplot(x='Type', y='Weekly_Sales', data=full_dff)
plt.title('Ventas Semanales por Tipo de Tienda')
plt.xlabel('Tipo de Tienda')
plt.ylabel('Ventas Semanales')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='IsHoliday', y='Weekly_Sales', data=full_dff)
plt.title('Ventas Semanales en Fiestas vs. No Fiestas')
plt.xlabel('Es Feriado')
plt.ylabel('Ventas Semanales')
plt.show()

plt.figure(figsize=(14, 8))
sns.boxplot(x='Dept', y='Weekly_Sales', data=full_dff)
plt.title('Ventas Semanales por Departamento')
plt.xlabel('Departamento')
plt.ylabel('Ventas Semanales')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Fuel_Price', y='Weekly_Sales', data=full_dff)
plt.title('Relación entre Precio del Combustible y Ventas Semanales')
plt.xlabel('Precio del Combustible')
plt.ylabel('Ventas Semanales')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Temperature', y='Weekly_Sales', data=full_dff)
plt.title('Relación entre Temperatura y Ventas Semanales')
plt.xlabel('Temperatura')
plt.ylabel('Ventas Semanales')
plt.show()

"""**Feature engineering**"""

full_dff = full_dff.query('Weekly_Sales > 0 and Weekly_Sales < 200000')
full_dff['Weekly_Sales'].describe()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

full_dff.info()

full_dff = pd.get_dummies(full_dff, columns=['Type', 'IsHoliday'], drop_first=True)

full_dff.info()

"""**SPlit de la data**"""

X = full_dff.drop(columns=['Weekly_Sales'])
y = full_dff['Weekly_Sales']

"""**Standart Scaler**"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""**SPlit para la regresion**"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

X_train.shape, y_train.shape

X_test.shape, y_test.shape

from sklearn.ensemble import RandomForestRegressor
# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# modelo
rf_model.fit(X_train, y_train)

# Hacer predicciones
y_pred = rf_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")